{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44ejk5yNrNpK",
        "outputId": "4a735f72-418f-4445-be6c-00f2cd69b09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voip-dns-test-march 100%[===================>] 310.72M  13.7MB/s    in 28s     \n",
            "voip-dns-train-marc 100%[===================>]  12.15G  13.8MB/s    in 16m 51s \n",
            "fullsubnet_best_mod 100%[===================>]  64.53M  10.4MB/s    in 6.2s    \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PREFIX\"] = \"wget -q https://cmu.box.com/shared/static\"\n",
        "os.environ[\"SUFFIX\"] = \"--content-disposition --show-progress\"\n",
        "\n",
        "!${PREFIX}/gwb1zmmrck3b5j5zbjpp303o6ij497at.zip ${SUFFIX}\n",
        "!unzip -q 11785-spring23-projects.zip\n",
        "\n",
        "!${PREFIX}/tblaokesvar8mr6r38n36pqrhqp9gyko.tar ${SUFFIX}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "11785-spring23-projects\n",
        "├── eval\n",
        "│   ├── radix\n",
        "│   │   └── dns\n",
        "│   │       └── source\n",
        "│   │           ├── radix_dns_source_clean\n",
        "│   │           └── radix_dns_source_noisy\n",
        "│   └── relay\n",
        "│       ├── slack\n",
        "│       │   └── cloud\n",
        "│       │       ├── src_slack_relay_auto\n",
        "│       │       └── src_slack_relay_low\n",
        "│       └── teams\n",
        "│           ├── cloud\n",
        "│           │   ├── src_teams_cloud_relay_auto\n",
        "│           │   └── src_teams_cloud_relay_low\n",
        "│           └── phone\n",
        "│               ├── src_teams_phone_relay_auto\n",
        "│               └── src_teams_phone_relay_low\n",
        "└── train\n",
        "    ├── radix\n",
        "    │   └── dns\n",
        "    │       └── source\n",
        "    │           ├── radix_dns_source_clean\n",
        "    │           └── radix_dns_source_noisy\n",
        "    └── relay\n",
        "        ├── slack\n",
        "        │   └── cloud\n",
        "        │       ├── relay_slack_auto\n",
        "        │       └── relay_slack_low\n",
        "        └── teams\n",
        "            ├── cloud\n",
        "            │   ├── relay_teams_cloud_high\n",
        "            │   └── relay_teams_cloud_low\n",
        "            └── phone\n",
        "                ├── relay_teams_phone_high\n",
        "                └── relay_teams_phone_low\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "N1ldd1A9lVUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/YunyangZeng/TAPLoss/archive/refs/heads/master.zip\n",
        "!unzip -q master.zip\n",
        "%cd /content/TAPLoss-master/FullSubNet/recipes/dns_interspeech_2020"
      ],
      "metadata": {
        "id": "v0nMFbOpFlHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4c189b-879f-4a35-cece-455857ca9619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-17 05:48:46--  https://github.com/YunyangZeng/TAPLoss/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/YunyangZeng/TAPLoss/zip/refs/heads/master [following]\n",
            "--2023-04-17 05:48:47--  https://codeload.github.com/YunyangZeng/TAPLoss/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [         <=>        ] 122.53M  13.4MB/s    in 9.1s    \n",
            "\n",
            "2023-04-17 05:48:56 (13.4 MB/s) - ‘master.zip’ saved [128486388]\n",
            "\n",
            "/content/TAPLoss-master/FullSubNet/recipes/dns_interspeech_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/TAPLoss-master/FullSubNet/recipes/dns_interspeech_2020/fullsubnet/trainer.py /content/TAPLoss-master/FullSubNet/"
      ],
      "metadata": {
        "id": "-PzH5LvIcdp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rich \n",
        "!pip install mir_eval \n",
        "!pip install git+https://github.com/ludlows/python-pesq#egg=pesq \n",
        "!pip install pypesq \n",
        "!pip install pystoi \n",
        "!pip install https://github.com/schmiph2/pysepm/archive/master.zip"
      ],
      "metadata": {
        "id": "B1B7Kdag4sfm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8d041ff-494a-41c2-a049-dcc5fde64bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.9/dist-packages (13.3.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich) (0.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mir_eval\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from mir_eval) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from mir_eval) (1.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from mir_eval) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from mir_eval) (1.16.0)\n",
            "Building wheels for collected packages: mir_eval\n",
            "  Building wheel for mir_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir_eval: filename=mir_eval-0.7-py3-none-any.whl size=100718 sha256=65f673817a5bc99b8d8a40c1b8f5a024b4919f2ebef5521862153f21e454cbf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/f5/d5/eb3db1d056253da195208853842bce745a84b29f44cab59b6c\n",
            "Successfully built mir_eval\n",
            "Installing collected packages: mir_eval\n",
            "Successfully installed mir_eval-0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pesq\n",
            "  Cloning https://github.com/ludlows/python-pesq to /tmp/pip-install-j19gl194/pesq_b0428e4133fd42a483e2477f86ab9946\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ludlows/python-pesq /tmp/pip-install-j19gl194/pesq_b0428e4133fd42a483e2477f86ab9946\n",
            "  Resolved https://github.com/ludlows/python-pesq to commit 5e230c59a3272fa80d8a6ea0d1e623f3fa560731\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pesq\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp39-cp39-linux_x86_64.whl size=257196 sha256=33ede4b7e9c03754ca6a28f42d7c83b05a80fe15f9bad16cf2683ec67df31f15\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b5bqbsb0/wheels/ce/46/42/cc11ff939a88c6aaa81cd563e0a791afd6c11a75cd51ba6f23\n",
            "Successfully built pesq\n",
            "Installing collected packages: pesq\n",
            "Successfully installed pesq-0.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypesq\n",
            "  Downloading pypesq-1.2.4.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pypesq) (1.22.4)\n",
            "Building wheels for collected packages: pypesq\n",
            "  Building wheel for pypesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypesq: filename=pypesq-1.2.4-cp39-cp39-linux_x86_64.whl size=94776 sha256=f7824b519d15f3b5feb155e917e1cd755778f95e4f30288b1148bfffe6a3ab89\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/68/ac/12aee270f3f864ef148078b59db149f562a4a0eeaa9c6358d0\n",
            "Successfully built pypesq\n",
            "Installing collected packages: pypesq\n",
            "Successfully installed pypesq-1.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pystoi\n",
            "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pystoi) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pystoi) (1.10.1)\n",
            "Building wheels for collected packages: pystoi\n",
            "  Building wheel for pystoi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7792 sha256=8227a98889899de374720e91d101acbdb3654c90197f8dff9ffa8b086a8b9e5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/b5/5b/ebb4c736d880f5726ad128c6a89b094ccb0eac84dac2c2ce88\n",
            "Successfully built pystoi\n",
            "Installing collected packages: pystoi\n",
            "Successfully installed pystoi-0.3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/schmiph2/pysepm/archive/master.zip\n",
            "  Downloading https://github.com/schmiph2/pysepm/archive/master.zip\n",
            "\u001b[2K     \u001b[32m|\u001b[0m \u001b[32m1.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pesq@ https://github.com/ludlows/python-pesq/archive/master.zip#egg=pesq\n",
            "  Downloading https://github.com/ludlows/python-pesq/archive/master.zip\n",
            "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m223.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy\n",
            "  Downloading https://github.com/jfsantos/SRMRpy/archive/master.zip\n",
            "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m39.3 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pysepm==0.1) (1.22.4)\n",
            "Requirement already satisfied: pystoi in /usr/local/lib/python3.9/dist-packages (from pysepm==0.1) (0.3.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from pysepm==0.1) (0.56.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pysepm==0.1) (1.10.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->pysepm==0.1) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->pysepm==0.1) (67.6.1)\n",
            "Collecting Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone\n",
            "  Downloading https://github.com/detly/gammatone/archive/master.zip\n",
            "\u001b[2K     \u001b[32m/\u001b[0m \u001b[32m59.4 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mock\n",
            "  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (3.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (5.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (4.39.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->Gammatone@ https://github.com/detly/gammatone/archive/master.zip#egg=Gammatone->SRMRpy@ https://github.com/jfsantos/SRMRpy/archive/master.zip#egg=SRMRpy->pysepm==0.1) (1.16.0)\n",
            "Building wheels for collected packages: pysepm, SRMRpy, Gammatone\n",
            "  Building wheel for pysepm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysepm: filename=pysepm-0.1-py3-none-any.whl size=24306 sha256=4d5735ce74d2496b382cd257d838979fb629d18dfa7753c6e50a66ce7076ad1c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nx09ea9d/wheels/28/ae/1d/19ccc0d05c4419b9d6441ad669c05a324b07a55db9a0ec7fe0\n",
            "  Building wheel for SRMRpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SRMRpy: filename=SRMRpy-1.0-py3-none-any.whl size=9387 sha256=f0a82e2da3830c9af58cff8391f3cb03c0995e2e42029575066040c74846d578\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nx09ea9d/wheels/55/b8/62/0617bc2cd230cd3f61597a9562c65d86c1011dcb98262b6c5e\n",
            "  Building wheel for Gammatone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Gammatone: filename=Gammatone-1.0-py3-none-any.whl size=21787 sha256=936b42396f29b251dc6d7e90a8769f8a44aa113f845b1cc602ec950437eeb5bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nx09ea9d/wheels/ba/de/25/e746071319d3ca267f491e29108942e52047c55606d8f6ccff\n",
            "Successfully built pysepm SRMRpy Gammatone\n",
            "Installing collected packages: nose, mock, Gammatone, SRMRpy, pysepm\n",
            "Successfully installed Gammatone-1.0 SRMRpy-1.0 mock-5.0.2 nose-1.3.7 pysepm-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_toml=\"\"\"\n",
        "\n",
        "[meta]\n",
        "save_dir = \"{}\"\n",
        "description = \"This is a description of FullSubNet finetuning with train partition 1.\"\n",
        "seed = 0  # set random seed for random, numpy, pytorch-gpu and pytorch-cpu\n",
        "use_amp = true\n",
        "cudnn_enable = false\n",
        "\n",
        "[acoustics]\n",
        "n_fft = 512\n",
        "win_length = 512\n",
        "sr = 16000\n",
        "hop_length = 256\n",
        "\n",
        "\n",
        "[loss_function]\n",
        "name = \"mse_loss\"\n",
        "\n",
        "[acoustic_loss]\n",
        "ac_loss_weight = 0 # change to 1 to use acoustic loss with weight 1\n",
        "ac_loss_only  = false\n",
        "model_path   = \"{}\"\n",
        "type = \"l1\"\n",
        "\n",
        "\n",
        "[loss_function.args]\n",
        "\n",
        "\n",
        "[optimizer]\n",
        "lr = 0.00001\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "\n",
        "[train_dataset]\n",
        "path = \"dataset_train.Dataset\"\n",
        "[train_dataset.args]\n",
        "clean_dataset = \"{}\"\n",
        "clean_dataset_limit = false\n",
        "clean_dataset_offset = 0\n",
        "noise_dataset = \"TAPLoss-master/FullSubNet/recipes/Datasets/noise.txt\"\n",
        "noise_dataset_limit = false\n",
        "noise_dataset_offset = 0\n",
        "num_workers =36\n",
        "pre_load_clean_dataset = false\n",
        "pre_load_noise = false\n",
        "pre_load_rir = false\n",
        "reverb_proportion = 0\n",
        "rir_dataset = \"TAPLoss-master/FullSubNet/recipes/Datasets/rir.txt\"\n",
        "rir_dataset_limit = false\n",
        "rir_dataset_offset = 0\n",
        "silence_length = 0\n",
        "snr_range = [-5, 20]\n",
        "sr = 16000\n",
        "sub_sample_length = 3.072\n",
        "target_dB_FS = -25\n",
        "target_dB_FS_floating_value = 10\n",
        "use_prepared_dataset = true\n",
        "noisy_dataset = \"{}\"\n",
        "\n",
        "\n",
        "[train_dataset.dataloader]\n",
        "batch_size = 8\n",
        "num_workers = 2\n",
        "drop_last = true\n",
        "pin_memory = false\n",
        "\n",
        "\n",
        "[validation_dataset]\n",
        "path = \"dataset_validation.Dataset\"\n",
        "[validation_dataset.args]\n",
        "dataset_dir_list = [\n",
        "    \"{}\"\n",
        "]\n",
        "sr = 16000\n",
        "\n",
        "\n",
        "[model]\n",
        "path = \"fullsubnet.model.Model\"\n",
        "\n",
        "[model.args]\n",
        "sb_num_neighbors = 15\n",
        "fb_num_neighbors = 0\n",
        "num_freqs = 257\n",
        "look_ahead = 2\n",
        "sequence_model = \"LSTM\"\n",
        "fb_output_activate_function = \"ReLU\"\n",
        "sb_output_activate_function = false\n",
        "fb_model_hidden_size = 512\n",
        "sb_model_hidden_size = 384\n",
        "weight_init = false\n",
        "norm_type = \"offline_laplace_norm\"\n",
        "num_groups_in_drop_band = 1\n",
        "\n",
        "\n",
        "[trainer]\n",
        "path = \"trainer.Trainer\"\n",
        "[trainer.train]\n",
        "clip_grad_norm_value = 1\n",
        "epochs = 10\n",
        "save_checkpoint_interval = 1\n",
        "[trainer.validation]\n",
        "save_max_metric_score = true\n",
        "validation_interval = 1\n",
        "[trainer.visualization]\n",
        "metrics = [\"WB_PESQ\", \"STOI\"]\n",
        "n_samples = 10\n",
        "num_workers = 4\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1itQX4IUMSeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_training_config(\n",
        "        training_toml_path        ,\n",
        "        train_noisy_txt           ,\n",
        "        test_noisy_path            ,\n",
        "        train_clean_txt           ,\n",
        "        test_clean_path            ,\n",
        "        eval_dir                  ,\n",
        "        learning_rate             ,\n",
        "        acoustic_loss_weight      ,\n",
        "        epochs                    ,\n",
        "        num_workers               ,\n",
        "        batch_size                ,\n",
        "        checkpoint):\n",
        "    \n",
        "    save_dir = \"/\".join(training_toml_path[:-5].split(\"/\")[:-1]) + \"/out\"\n",
        "\n",
        "    try:\n",
        "      os.mkdir(save_dir)\n",
        "    except:\n",
        "      pass \n",
        "      \n",
        "    train_toml2 = train_toml.format( \n",
        "        save_dir,\n",
        "        checkpoint,\n",
        "        train_clean_txt,\n",
        "        train_noisy_txt,\n",
        "        eval_dir\n",
        "     )\n",
        "\n",
        "    with open(training_toml_path, \"w\") as f:\n",
        "        f.write(train_toml2)"
      ],
      "metadata": {
        "id": "84TZGufAE6PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_training_runner(\n",
        "    training_voip_runner_path ,\n",
        "    training_voip_config_path ,\n",
        "    checkpoint                ):\n",
        "\n",
        "    nnodes         = 1\n",
        "    nproc_per_node = 1\n",
        "    train_toml     = training_voip_config_path\n",
        "\n",
        "    training_voip_runner_text = \"\"\"\n",
        "    #!/bin/bash\n",
        "\n",
        "    torchrun                \\\n",
        "        --standalone        \\\n",
        "        --nnodes={}         \\\n",
        "        --nproc_per_node={} \\\n",
        "        train.py            \\\n",
        "        -C {}               \\\n",
        "        -P {}\n",
        "    \"\"\".format(\n",
        "        nnodes         ,\n",
        "        nproc_per_node ,\n",
        "        train_toml     ,\n",
        "        checkpoint     )\n",
        "\n",
        "    with open(training_voip_runner_path, \"w\") as f:\n",
        "        f.write(training_voip_runner_text)\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "7QZLe_Ab7HYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_dir_struct(\n",
        "        test_noisy_path,\n",
        "        test_clean_path):\n",
        "  \n",
        "    eval_dir = \"/\".join(test_noisy_path.split(\"/\")[:-1]) + \"/no_reverb/\"\n",
        "    if not os.path.isdir(eval_dir):\n",
        "      os.mkdir(eval_dir)\n",
        "      os.mkdir(f\"{eval_dir}/noisy\")\n",
        "      os.mkdir(f\"{eval_dir}/clean\")\n",
        "      os.system(f\"cp -r {test_noisy_path}/*.wav {eval_dir}/noisy\")\n",
        "      os.system(f\"cp -r {test_clean_path}/*.wav {eval_dir}/clean\")\n",
        "    return eval_dir\n",
        "\n",
        "def setupTrainingRunner(\n",
        "            training_runner_path     ,\n",
        "            training_toml_path        ,\n",
        "            train_noisy_txt           ,\n",
        "            test_noisy_path           ,\n",
        "            train_clean_txt           ,\n",
        "            test_clean_path           ,\n",
        "            learning_rate             ,\n",
        "            acoustic_loss_weight      ,\n",
        "            epochs                    ,\n",
        "            num_workers               ,\n",
        "            batch_size                ,\n",
        "            checkpoint                ):\n",
        "\n",
        " \n",
        "    eval_dir = make_eval_dir_struct(\n",
        "        test_noisy_path,\n",
        "        test_clean_path\n",
        "    )\n",
        "\n",
        "    make_training_config(\n",
        "        training_toml_path        ,\n",
        "        train_noisy_txt           ,\n",
        "        test_noisy_path           ,\n",
        "        train_clean_txt           ,\n",
        "        test_clean_path           ,\n",
        "        eval_dir                  ,\n",
        "        learning_rate             ,\n",
        "        acoustic_loss_weight      ,\n",
        "        epochs                    ,\n",
        "        num_workers               ,\n",
        "        batch_size                ,\n",
        "        checkpoint                )\n",
        "\n",
        "\n",
        "    make_training_runner(\n",
        "        training_runner_path ,\n",
        "        training_toml_path ,\n",
        "        checkpoint                )\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "VcLn14_ApYZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainPartitions(\n",
        "    training_toml_paths,\n",
        "    train_noisy_txts    ,\n",
        "    test_noisy_paths     ,\n",
        "    train_clean_txt     ,\n",
        "    test_clean_path      ,\n",
        "    learning_rate        ,\n",
        "    acoustic_loss_weight ,\n",
        "    epochs               ,\n",
        "    num_workers          ,\n",
        "    batch_size           ,\n",
        "    checkpoint           ):\n",
        "\n",
        "    for (train_noisy_txt, test_noisy_path, training_toml_path) in zip(train_noisy_txts, test_noisy_paths, training_toml_paths):\n",
        "\n",
        "        training_runner_path = training_toml_path[:-5] + \".sh\"\n",
        "        setupTrainingRunner(\n",
        "            training_runner_path     ,\n",
        "            training_toml_path        ,\n",
        "            train_noisy_txt           ,\n",
        "            test_noisy_path           ,\n",
        "            train_clean_txt           ,\n",
        "            test_clean_path           ,\n",
        "            learning_rate             ,\n",
        "            acoustic_loss_weight      ,\n",
        "            epochs                    ,\n",
        "            num_workers               ,\n",
        "            batch_size                ,\n",
        "            checkpoint                )\n",
        "        print(training_runner_path)\n",
        "        os.environ[\"TRAINING_RUNNER_PATH\"] = training_runner_path\n",
        "        !sh {training_runner_path}\n",
        "        break\n"
      ],
      "metadata": {
        "id": "MAj7Zz3ArEk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "GET_IDS = lambda p: int(re.split('_|\\.', p.split(\"fileid_\")[1])[0])\n",
        "\n",
        "train_clean_path =\\\n",
        "    \"/content/11785-spring23-projects/train/radix/dns/source/radix_dns_source_clean\"\n",
        "\n",
        "test_clean_path =\\\n",
        "    \"/content/11785-spring23-projects/test/radix/dns/source/radix_dns_source_clean\"\n",
        "\n",
        "train_noisy_paths = [\n",
        "    \"/content/11785-spring23-projects/train/relay/teams/cloud/relay_teams_cloud_auto\",\n",
        "    \"/content/11785-spring23-projects/train/relay/teams/cloud/relay_teams_cloud_low\" ,\n",
        "    \"/content/11785-spring23-projects/train/relay/teams/phone/relay_teams_phone_auto\",\n",
        "    \"/content/11785-spring23-projects/train/relay/teams/phone/relay_teams_phone_low\" ,\n",
        "    \"/content/11785-spring23-projects/train/relay/slack/cloud/relay_slack_auto\"  ,\n",
        "    \"/content/11785-spring23-projects/train/relay/slack/cloud/relay_slack_low\"   ]\n",
        "\n",
        "test_noisy_paths = [\n",
        "    \"/content/11785-spring23-projects/eval/relay/teams/cloud/relay_teams_cloud_auto\",\n",
        "    \"/content/11785-spring23-projects/eval/relay/teams/cloud/relay_teams_cloud_low\" ,\n",
        "    \"/content/11785-spring23-projects/eval/relay/teams/phone/relay_teams_phone_auto\",\n",
        "    \"/content/11785-spring23-projects/eval/relay/teams/phone/relay_teams_phone_low\" ,\n",
        "    \"/content/11785-spring23-projects/eval/relay/slack/cloud/relay_slack_auto\"  ,\n",
        "    \"/content/11785-spring23-projects/eval/relay/slack/cloud/relay_slack_low\"   ]\n",
        "\n",
        "training_toml_paths = [name + \".toml\" for name in train_noisy_paths]\n",
        "\n",
        "train_noisy_txts = [name + \".txt\" for name in train_noisy_paths]\n",
        "train_clean_txt = train_clean_path + \".txt\"\n",
        "\n",
        "for path in [train_clean_path] + train_noisy_paths:\n",
        "    audio_paths = sorted(glob.glob(f\"{path}/*\"), key = GET_IDS)\n",
        "    with open(f\"{path}.txt\", 'w') as f:\n",
        "        for line in audio_paths:\n",
        "            f.write(line)\n",
        "            f.write('\\n')"
      ],
      "metadata": {
        "id": "6UPPTN4N3KFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f828f6f3-3b40-411c-f5da-55f8e71ed608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eea74dd3d646>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_clean_path\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_noisy_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0maudio_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGET_IDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/11785-spring23-projects/train/radix/dns/source/radix_dns_source_clean.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base = \"/content/\"\n",
        "\n",
        "learning_rate        = 1e-4\n",
        "acoustic_loss_weight = 0.0\n",
        "stft_loss_weight     = 1.0\n",
        "epochs               = 10\n",
        "num_workers          = 2\n",
        "batch_size           = 10\n",
        "\n",
        "checkpoint = base + \"fullsubnet_best_model_58epochs.tar\"\n",
        "\n",
        "trainPartitions(\n",
        "    training_toml_paths  ,\n",
        "    train_noisy_txts     ,\n",
        "    test_noisy_paths     ,\n",
        "    train_clean_txt      ,\n",
        "    test_clean_path      ,\n",
        "    learning_rate        ,\n",
        "    acoustic_loss_weight ,\n",
        "    epochs               ,\n",
        "    num_workers          ,\n",
        "    batch_size           ,\n",
        "    checkpoint           )"
      ],
      "metadata": {
        "id": "7Ccfx4HMHSiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba15ec2a-cfef-4541-f30f-b2fadecab5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/voip-dns-test-march2/relay/gmeet/cloud/no_reverb/\n",
            "if\n",
            "\n",
            "\n",
            "[meta]\n",
            "save_dir = \"/content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto/out\"\n",
            "description = \"This is a description of FullSubNet finetuning with train partition 1.\"\n",
            "seed = 0  # set random seed for random, numpy, pytorch-gpu and pytorch-cpu\n",
            "use_amp = true\n",
            "cudnn_enable = false\n",
            "\n",
            "[acoustics]\n",
            "n_fft = 512\n",
            "win_length = 512\n",
            "sr = 16000\n",
            "hop_length = 256\n",
            "\n",
            "\n",
            "[loss_function]\n",
            "name = \"mse_loss\"\n",
            "\n",
            "[acoustic_loss]\n",
            "ac_loss_weight = 0 # change to 1 to use acoustic loss with weight 1\n",
            "ac_loss_only  = false\n",
            "model_path   = \"/content/fullsubnet_best_model_58epochs.tar\"\n",
            "type = \"l1\"\n",
            "\n",
            "\n",
            "[loss_function.args]\n",
            "\n",
            "\n",
            "[optimizer]\n",
            "lr = 0.00001\n",
            "beta1 = 0.9\n",
            "beta2 = 0.999\n",
            "\n",
            "\n",
            "[train_dataset]\n",
            "path = \"dataset_train.Dataset\"\n",
            "[train_dataset.args]\n",
            "clean_dataset = \"/content/voip-dns-train-march2/radix/dns/source/radix_dns_source_clean.txt\"\n",
            "clean_dataset_limit = false\n",
            "clean_dataset_offset = 0\n",
            "noise_dataset = \"TAPLoss-master/FullSubNet/recipes/Datasets/noise.txt\"\n",
            "noise_dataset_limit = false\n",
            "noise_dataset_offset = 0\n",
            "num_workers =36\n",
            "pre_load_clean_dataset = false\n",
            "pre_load_noise = false\n",
            "pre_load_rir = false\n",
            "reverb_proportion = 0\n",
            "rir_dataset = \"TAPLoss-master/FullSubNet/recipes/Datasets/rir.txt\"\n",
            "rir_dataset_limit = false\n",
            "rir_dataset_offset = 0\n",
            "silence_length = 0\n",
            "snr_range = [-5, 20]\n",
            "sr = 16000\n",
            "sub_sample_length = 3.072\n",
            "target_dB_FS = -25\n",
            "target_dB_FS_floating_value = 10\n",
            "use_prepared_dataset = true\n",
            "noisy_dataset = \"/content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto.txt\"\n",
            "\n",
            "\n",
            "[train_dataset.dataloader]\n",
            "batch_size = 8\n",
            "num_workers = 2\n",
            "drop_last = true\n",
            "pin_memory = false\n",
            "\n",
            "\n",
            "[validation_dataset]\n",
            "path = \"dataset_validation.Dataset\"\n",
            "[validation_dataset.args]\n",
            "dataset_dir_list = [\n",
            "    \"/content/voip-dns-test-march2/relay/gmeet/cloud/no_reverb/\"\n",
            "]\n",
            "sr = 16000\n",
            "\n",
            "\n",
            "[model]\n",
            "path = \"fullsubnet.model.Model\"\n",
            "\n",
            "[model.args]\n",
            "sb_num_neighbors = 15\n",
            "fb_num_neighbors = 0\n",
            "num_freqs = 257\n",
            "look_ahead = 2\n",
            "sequence_model = \"LSTM\"\n",
            "fb_output_activate_function = \"ReLU\"\n",
            "sb_output_activate_function = false\n",
            "fb_model_hidden_size = 512\n",
            "sb_model_hidden_size = 384\n",
            "weight_init = false\n",
            "norm_type = \"offline_laplace_norm\"\n",
            "num_groups_in_drop_band = 1\n",
            "\n",
            "\n",
            "[trainer]\n",
            "path = \"trainer.Trainer\"\n",
            "[trainer.train]\n",
            "clip_grad_norm_value = 1\n",
            "epochs = 10\n",
            "save_checkpoint_interval = 1\n",
            "[trainer.validation]\n",
            "save_max_metric_score = true\n",
            "validation_interval = 1\n",
            "[trainer.visualization]\n",
            "metrics = [\"WB_PESQ\", \"STOI\"]\n",
            "n_samples = 10\n",
            "num_workers = 4\n",
            "\n",
            "\n",
            "    #!/bin/bash\n",
            "\n",
            "    torchrun                        --standalone                --nnodes=1                 --nproc_per_node=1         train.py                    -C /content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto.toml                       -P /content/fullsubnet_best_model_58epochs.tar\n",
            "    \n",
            "/content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto.sh\n",
            "1 process initialized.\n",
            "Model preloaded successfully from \u001b[35m/content/\u001b[0m\u001b[95mfullsubnet_best_model_58epochs.tar.\u001b[0m\n",
            "2023-03-05 03:58:34.627858: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-05 03:58:35.996310: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-05 03:58:35.996496: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-05 03:58:35.996523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "The configurations are as follows: \n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'meta'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'save_dir'\u001b[0m: \n",
            "\u001b[32m'/content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto/out'\u001b[0m,\n",
            "        \u001b[32m'description'\u001b[0m: \u001b[32m'This is a description of FullSubNet finetuning with \u001b[0m\n",
            "\u001b[32mtrain partition 1.'\u001b[0m,\n",
            "        \u001b[32m'seed'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "        \u001b[32m'use_amp'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
            "        \u001b[32m'cudnn_enable'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "        \u001b[32m'experiment_name'\u001b[0m: \u001b[32m'relay_gmeet_cloud_auto'\u001b[0m,\n",
            "        \u001b[32m'config_path'\u001b[0m: \n",
            "\u001b[32m'/content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto.toml'\u001b[0m,\n",
            "        \u001b[32m'preloaded_model_path'\u001b[0m: \u001b[32m'/content/fullsubnet_best_model_58epochs.tar'\u001b[0m\n",
            "    \u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'acoustics'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'n_fft'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
            "        \u001b[32m'win_length'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
            "        \u001b[32m'sr'\u001b[0m: \u001b[1;36m16000\u001b[0m,\n",
            "        \u001b[32m'hop_length'\u001b[0m: \u001b[1;36m256\u001b[0m\n",
            "    \u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'loss_function'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'mse_loss'\u001b[0m, \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'acoustic_loss'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'ac_loss_weight'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "        \u001b[32m'ac_loss_only'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "        \u001b[32m'model_path'\u001b[0m: \u001b[32m'/content/fullsubnet_best_model_58epochs.tar'\u001b[0m,\n",
            "        \u001b[32m'type'\u001b[0m: \u001b[32m'l1'\u001b[0m\n",
            "    \u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'lr'\u001b[0m: \u001b[1;36m1e-05\u001b[0m, \u001b[32m'beta1'\u001b[0m: \u001b[1;36m0.9\u001b[0m, \u001b[32m'beta2'\u001b[0m: \u001b[1;36m0.999\u001b[0m\u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'train_dataset'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'path'\u001b[0m: \u001b[32m'dataset_train.Dataset'\u001b[0m,\n",
            "        \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'clean_dataset'\u001b[0m: \n",
            "\u001b[32m'/content/voip-dns-train-march2/radix/dns/source/radix_dns_source_clean.txt'\u001b[0m,\n",
            "            \u001b[32m'clean_dataset_limit'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'clean_dataset_offset'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "            \u001b[32m'noise_dataset'\u001b[0m: \n",
            "\u001b[32m'TAPLoss-master/FullSubNet/recipes/Datasets/noise.txt'\u001b[0m,\n",
            "            \u001b[32m'noise_dataset_limit'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'noise_dataset_offset'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "            \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m36\u001b[0m,\n",
            "            \u001b[32m'pre_load_clean_dataset'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'pre_load_noise'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'pre_load_rir'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'reverb_proportion'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "            \u001b[32m'rir_dataset'\u001b[0m: \u001b[32m'TAPLoss-master/FullSubNet/recipes/Datasets/rir.txt'\u001b[0m,\n",
            "            \u001b[32m'rir_dataset_limit'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'rir_dataset_offset'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "            \u001b[32m'silence_length'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "            \u001b[32m'snr_range'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m-5\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m,\n",
            "            \u001b[32m'sr'\u001b[0m: \u001b[1;36m16000\u001b[0m,\n",
            "            \u001b[32m'sub_sample_length'\u001b[0m: \u001b[1;36m3.072\u001b[0m,\n",
            "            \u001b[32m'target_dB_FS'\u001b[0m: \u001b[1;36m-25\u001b[0m,\n",
            "            \u001b[32m'target_dB_FS_floating_value'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
            "            \u001b[32m'use_prepared_dataset'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
            "            \u001b[32m'noisy_dataset'\u001b[0m: \n",
            "\u001b[32m'/content/voip-dns-train-march2/relay/gmeet/cloud/relay_gmeet_cloud_auto.txt'\u001b[0m\n",
            "        \u001b[1m}\u001b[0m,\n",
            "        \u001b[32m'dataloader'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
            "            \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
            "            \u001b[32m'drop_last'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
            "            \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
            "        \u001b[1m}\u001b[0m\n",
            "    \u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'validation_dataset'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'path'\u001b[0m: \u001b[32m'dataset_validation.Dataset'\u001b[0m,\n",
            "        \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'dataset_dir_list'\u001b[0m: \u001b[1m[\u001b[0m\n",
            "                \u001b[32m'/content/voip-dns-test-march2/relay/gmeet/cloud/no_reverb/'\u001b[0m\n",
            "            \u001b[1m]\u001b[0m,\n",
            "            \u001b[32m'sr'\u001b[0m: \u001b[1;36m16000\u001b[0m\n",
            "        \u001b[1m}\u001b[0m\n",
            "    \u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'model'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'path'\u001b[0m: \u001b[32m'fullsubnet.model.Model'\u001b[0m,\n",
            "        \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'sb_num_neighbors'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
            "            \u001b[32m'fb_num_neighbors'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
            "            \u001b[32m'num_freqs'\u001b[0m: \u001b[1;36m257\u001b[0m,\n",
            "            \u001b[32m'look_ahead'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
            "            \u001b[32m'sequence_model'\u001b[0m: \u001b[32m'LSTM'\u001b[0m,\n",
            "            \u001b[32m'fb_output_activate_function'\u001b[0m: \u001b[32m'ReLU'\u001b[0m,\n",
            "            \u001b[32m'sb_output_activate_function'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'fb_model_hidden_size'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
            "            \u001b[32m'sb_model_hidden_size'\u001b[0m: \u001b[1;36m384\u001b[0m,\n",
            "            \u001b[32m'weight_init'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
            "            \u001b[32m'norm_type'\u001b[0m: \u001b[32m'offline_laplace_norm'\u001b[0m,\n",
            "            \u001b[32m'num_groups_in_drop_band'\u001b[0m: \u001b[1;36m1\u001b[0m\n",
            "        \u001b[1m}\u001b[0m\n",
            "    \u001b[1m}\u001b[0m,\n",
            "    \u001b[32m'trainer'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "        \u001b[32m'path'\u001b[0m: \u001b[32m'trainer.Trainer'\u001b[0m,\n",
            "        \u001b[32m'train'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'clip_grad_norm_value'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
            "            \u001b[32m'epochs'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
            "            \u001b[32m'save_checkpoint_interval'\u001b[0m: \u001b[1;36m1\u001b[0m\n",
            "        \u001b[1m}\u001b[0m,\n",
            "        \u001b[32m'validation'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'save_max_metric_score'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
            "            \u001b[32m'validation_interval'\u001b[0m: \u001b[1;36m1\u001b[0m\n",
            "        \u001b[1m}\u001b[0m,\n",
            "        \u001b[32m'visualization'\u001b[0m: \u001b[1m{\u001b[0m\n",
            "            \u001b[32m'metrics'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'WB_PESQ'\u001b[0m, \u001b[32m'STOI'\u001b[0m\u001b[1m]\u001b[0m,\n",
            "            \u001b[32m'n_samples'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
            "            \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m4\u001b[0m\n",
            "        \u001b[1m}\u001b[0m\n",
            "    \u001b[1m}\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "This project contains \u001b[1;36m1\u001b[0m models, the number of the parameters is: \n",
            "        Network \u001b[1;36m1\u001b[0m: \u001b[1;36m5.637635\u001b[0m million.\n",
            "The amount of parameters in the project is \u001b[1;36m5.637635\u001b[0m million.\n",
            "=============== \u001b[1;36m1\u001b[0m epoch ===============\n",
            "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m seconds\u001b[1m]\u001b[0m Begin training\u001b[33m...\u001b[0m\n",
            "Training: 100% 150/150 [01:32<00:00,  1.61it/s]\n",
            "         Saving \u001b[1;36m1\u001b[0m epoch model checkpoint\u001b[33m...\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[1;36m93\u001b[0m seconds\u001b[1m]\u001b[0m Training has finished, validation is in progress\u001b[33m...\u001b[0m\n",
            "Validation: 150it [00:59,  2.52it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "\n",
            "Y\n",
            "         Saving \u001b[1;36m1\u001b[0m epoch model checkpoint\u001b[33m...\u001b[0m\n",
            "         😃 Found a best score in the \u001b[1;36m1\u001b[0m epoch, saving\u001b[33m...\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[1;36m259\u001b[0m seconds\u001b[1m]\u001b[0m This epoch is finished.\n",
            "=============== \u001b[1;36m2\u001b[0m epoch ===============\n",
            "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m seconds\u001b[1m]\u001b[0m Begin training\u001b[33m...\u001b[0m\n",
            "Training:  11% 16/150 [00:09<01:21,  1.64it/s]WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17974 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Training:  11% 16/150 [00:10<01:28,  1.51it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 101, in <module>\n",
            "    entry(local_rank, configuration, args.resume, args.only_validation)\n",
            "  File \"train.py\", line 74, in entry\n",
            "    trainer.train()\n",
            "  File \"/content/TAPLoss-master/FullSubNet/audio_zen/trainer/base_trainer.py\", line 350, in train\n",
            "    self._train_epoch(epoch)\n",
            "  File \"/content/TAPLoss-master/FullSubNet/trainer.py\", line 78, in _train_epoch\n",
            "    self.scaler.scale(loss).backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 488, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py\", line 762, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py\", line 753, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py\", line 237, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 850, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 17960 got signal: 2\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}